<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>common-methods-of-matplotlib</title>
    <url>/common-methods-of-matplotlib.html</url>
    <content><![CDATA[<p>本文创建子图的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = plt.figure(figsize=[<span class="number">8</span>, <span class="number">6</span>], tight_layout=<span class="literal">True</span>)  </span><br><span class="line">ax = fig.subplots(<span class="number">2</span>, <span class="number">3</span>, sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="标题-title"><a href="#标题-title" class="headerlink" title="标题 title"></a>标题 title</h1><h2 id="为每个子图设置一个标题"><a href="#为每个子图设置一个标题" class="headerlink" title="为每个子图设置一个标题"></a>为每个子图设置一个标题</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax[i, j].set_title(<span class="string">&#x27;title&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="设置主标题"><a href="#设置主标题" class="headerlink" title="设置主标题"></a>设置主标题</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig.suptitle(<span class="string">&#x27;suptitle&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="图例-legend"><a href="#图例-legend" class="headerlink" title="图例 legend"></a>图例 legend</h1><h2 id="为所有子图创建一个图例"><a href="#为所有子图创建一个图例" class="headerlink" title="为所有子图创建一个图例"></a>为所有子图创建一个图例</h2><p>使用matplotlib创建子图后，如果所有子图的图例都一样，可以使用<a href="https://www.delftstack.com/zh/howto/matplotlib/how-to-make-a-single-legend-for-all-subplots-in-matplotlib/">这篇文章</a>中提到的方法只显示一个图例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lines, labels = fig.axes[-<span class="number">1</span>].get_legend_handles_labels()    </span><br><span class="line">fig.legend(lines, labels, loc = <span class="string">&#x27;upper center&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="在图外显示图例"><a href="#在图外显示图例" class="headerlink" title="在图外显示图例"></a>在图外显示图例</h2><p>图例可以通过使用<code>bbox_to_anchor</code>放置在 Matplotlib 中的绘图之外。使用示例参考<a href="https://www.delftstack.com/zh/howto/matplotlib/how-to-place-legend-outside-of-the-plot-in-matplotlib/">这篇文章</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig.legend(bbox_to_anchor=(<span class="number">1.05</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title>powershell中无法使用conda activate</title>
    <url>/how-to-activate-conda-environment-from-powershell.html</url>
    <content><![CDATA[<p>环境：win10</p>
<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>powershell中无法使用conda activate</p>
<span id="more"></span>
<h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><p>在<code>cmd.exe</code>中运行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda init powershell</span><br></pre></td></tr></table></figure>
<p>然后重启<code>powershell</code>，就可以使用<code>conda activate</code>激活环境了。<br><strong>注意</strong>：在打开终端时，conda会默认会自动激活，可以通过执行以下命令取消自动激活。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda config --set auto_activate_base false</span><br></pre></td></tr></table></figure>

<p>【参考资料】</p>
<ol>
<li><a href="https://stackoverflow.com/questions/64149680/how-to-activate-conda-environment-from-powershell">How to activate conda environment from powershell?</a></li>
</ol>
]]></content>
      <categories>
        <category>问题记录</category>
      </categories>
      <tags>
        <tag>powershell</tag>
        <tag>conda</tag>
      </tags>
  </entry>
  <entry>
    <title>keras学习笔记一</title>
    <url>/learn-keras1.html</url>
    <content><![CDATA[<p>Tensorflow Version:  2.4.0</p>
<p><strong>训练神经网络应该使用keras还是tf.keras？</strong></p>
<p>从keras2.3.0版本开始，keras已经和tf.keras同步，但是开发人员应该在TensorFlow2.0上使用<strong>tf.keras</strong>，因为后续的keras软件包将仅支持错误修复，且后端将仅支持tensorflow。$^{[1]}$。</p>
<span id="more"></span>
<h1 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h1><p>TensorFlow Datasets 提供了一系列可以和 TensorFlow 配合使用的数据集。</p>
<ol>
<li>查看tensorflow支持的数据集列表：<a href="https://www.tensorflow.org/datasets/catalog/overview?hl=zh-cn#all_datasets">数据集文档</a>。</li>
<li>使用tensorflow数据集：<a href="https://www.tensorflow.org/datasets/overview?hl=zh-cn">开始使用</a>。</li>
</ol>
<p>使用<code>tfds.load</code>加载数据集，首次加载数据集会将数据集下载到本地，可以通过指定 <code>data_dir=</code> (默认是 <code>~/tensorflow_datasets/</code>) 来自定义数据保存/加载的路径。数据首次加载完成后，再调用这个接口就不会重复下载数据了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"></span><br><span class="line">mnist_train = tfds.load(name=<span class="string">&quot;mnist&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/89017996">Keras vs tf.keras: 在TensorFlow 2.0中有什么区别?</a></li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>keras</category>
      </categories>
      <tags>
        <tag>keras</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习笔记——Auto-Encoder</title>
    <url>/lhy-auto-encoder.html</url>
    <content><![CDATA[<p>Auto-Encoder是一种非监督学习，它的主要作用是对输入进行压缩表示。</p>
<span id="more"></span>

<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><h2 id="数据降维"><a href="#数据降维" class="headerlink" title="数据降维"></a>数据降维</h2><p><img src="data:image/png;base64,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" data-original="./auto_encoder.jpg" alt="Auto-Encoder"><br>具体的做法：</p>
<ol>
<li>使用一个Encoder网络对图片进行编码，得到一个code。这个code也称做<strong>Embedding、Latent Respresentation、Latent Code</strong>。</li>
<li>再使用一个Decoder网络对code进行解码，得到一个图片。</li>
<li>将Encoder和Decoder同时进行训练。<h3 id="相关技术-PCA"><a href="#相关技术-PCA" class="headerlink" title="相关技术-PCA"></a>相关技术-PCA</h3></li>
<li>输入一个图片$x$（输入前会先做正则化，处理成均值为0方差为1的分布）。</li>
<li>$x$乘上一个权重矩阵（$W$）得到$c$。</li>
<li>$c$再乘上一个权重矩阵（$W^T$）得到$\hat{x}$。</li>
</ol>
<p><img src="data:image/png;base64,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" data-original="./pca.jpg" alt="pca"><br>在PCA中，目标是最小化$(x - \hat{x})^2$，使得$x$和$\hat{x}$越接近越好，$c$是我们最终需要的。</p>
<p>如果把PCA当作神经网络来看，$x$相当于Input layer，$\hat{x}$相当于Output layer，$c$相当于hidden layer，$c$在PCA中是线性的，通常又叫做Bottleneck layer（因为$c$的维度通常比输入维度小得多）。$x \rightarrow c$的过程就是encode，$c \rightarrow \hat{x}$的过程就是decode。</p>
<h2 id="文本搜索"><a href="#文本搜索" class="headerlink" title="文本搜索"></a>文本搜索</h2><p>使用VSM(Vector Space Model)搜索带有某个词汇的文章，工作过程：</p>
<ol>
<li>把<strong>每篇文章</strong>都表示成空间中的一个点。</li>
<li>把<strong>查询词汇</strong>也表示成一个点。</li>
<li>计算查询词汇跟每篇文章的<strong>距离</strong>（余弦相似度等），取其中距离较小的文章。</li>
</ol>
<p>VSM中将文章表示为向量的方法：Bag-of-word。缺点：表示文章的向量中没有语义信息。</p>
<p>使用Auto-encoder将每篇文章表示成一个二维向量，可以将<strong>语义</strong>的信息考虑进来。<br><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="./text_retrieve.jpg" alt="text retrieve"></p>
<h2 id="相似图片搜索-1"><a href="#相似图片搜索-1" class="headerlink" title="相似图片搜索$^{[1]}$"></a>相似图片搜索$^{[1]}$</h2><p>简单的做法：使用欧式距离在像素级别上计算两张图片的相似度。（结果不好）</p>
<p>使用Auto-encoder把每张图片表示成一个code，在code上计算相似度的结果就会比较好。<br><img src="data:image/png;base64,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" data-original="./image_retrieve.jpg" alt="image retrieve"></p>
<h2 id="预训练DNN"><a href="#预训练DNN" class="headerlink" title="预训练DNN"></a>预训练DNN</h2><p>训练DNN的时候可以通过预训练寻找一组比较好的参数。</p>
<p>使用Auto-encoder预训练的做法：</p>
<ol>
<li>训练一个Auto-encoder，输入是784维，code是1000维，输出是784维，训练好之后把$W^1$保存下来。<mark>code的维度比input的维度大的时候要注意auto-encoder直接把input数据拿过来的问题。</mark></li>
<li>训练一个Auto-encoder，输入是1000维，code是1000维，输出是1000维，训练好之后把$W^2$保存下来。</li>
<li>训练一个Auto-encoder，输入是1000维，code是500维，输出是1000维，训练好之后把$W^3$保存下来。</li>
<li>随机初始化500到100的权重矩阵。</li>
<li>使用backpropagation进行微调（Fine-tune）。<br><img src="data:image/png;base64,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" data-original="./pre_train.jpg" alt="pre training"><br>现在的神经网络大多不需要预训练也可以训练的很好。但是当数据集中有<mark>大量的无标签数据，只有少量的带标签数据</mark>的时候，就可以用到预训练，先用大量的无标签数据把$W^1$、$W^2$和$W^3$先训练好，然后使用带标签数据进行微调。</li>
</ol>
<h1 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h1><h2 id="De-noising-auto-encoder-2"><a href="#De-noising-auto-encoder-2" class="headerlink" title="De-noising auto-encoder$^{[2]}$"></a>De-noising auto-encoder$^{[2]}$</h2><p>把原来的输入加上一些噪声后再做encode和decode。<br><img src="data:image/png;base64,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" data-original="./de_noising_ae.jpg" alt="De-noising auto-encoder"><br>encoder不仅学习到了code，还学到了过滤噪声的能力。</p>
<h2 id="Contractive-auto-encoder（CAE）-3"><a href="#Contractive-auto-encoder（CAE）-3" class="headerlink" title="Contractive auto-encoder（CAE）$^{[3]}$"></a>Contractive auto-encoder（CAE）$^{[3]}$</h2><p>在学习code的时候加上一个规则，用以最小化输入的变化对code的影响。</p>
<h1 id="Auto-encoder-for-CNN"><a href="#Auto-encoder-for-CNN" class="headerlink" title="Auto-encoder for CNN"></a>Auto-encoder for CNN</h1><p>CNN处理图片的时候，会用卷积（Convolution）和池化（Pooling）操作让图片变得越来越小，要用auto-encoder的话，不仅需要encoder，还需要decoder，如果encoder是做Convolution和Pooling，那么decoder要做的就是 Deconvolution 和 Unpooling。</p>
<h2 id="Unpooling"><a href="#Unpooling" class="headerlink" title="Unpooling"></a>Unpooling</h2><p>在pooling的时候记录下选择的像素点的位置，Unpooling的时候把除了此像素点之外的像素还原为0。</p>
<p>keras中的做法：pooling的时候不记录位置，还原的时候将所有像素点还原为一样的值。</p>
<h2 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h2><p>实际上，Deconvolution 就是 Convolution。<br><img src="data:image/png;base64,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" data-original="./deconvolution.jpg" alt="Deconvolution"><br>Deconvolution相当于先做padding，再做Convolution。</p>
<h2 id="sequence-to-sequence-Auto-encoder"><a href="#sequence-to-sequence-Auto-encoder" class="headerlink" title="sequence-to-sequence Auto-encoder"></a>sequence-to-sequence Auto-encoder</h2><p>decoder可以用来<strong>generate</strong>文本或者图片。</p>
<h1 id="评估Encoder的方法"><a href="#评估Encoder的方法" class="headerlink" title="评估Encoder的方法"></a>评估Encoder的方法</h1><p>之前评估Encoder的方法是，训练Encoder时也训练一个Decoder，然后通过最小化原始数据与Decoder重构出来的数据的差异，得到一个好的code。</p>
<p>这里介绍一个新的方法：</p>
<ol>
<li>将原始数据和embedding一起输入到一个二元分类器中，这个二元分类器会判断原始数据和embedding是否有对应关系。</li>
<li>最小化二元分类器的loss。loss比较小则说明AE训练出来的embedding是具有代表性的。反之，则说明这个embedding不具有代表性。</li>
</ol>
<hr>
<ol>
<li>Auto-encoder在<strong>图片搜索</strong>上的应用：Krizhevsky A, Hinton G E. Using very deep autoencoders for content-based image retrieval[C]//ESANN. 2011, 1: 2.</li>
<li>Vincent P, Larochelle H, Bengio Y, et al. Extracting and composing robust features with denoising autoencoders[C]//Proceedings of the 25th international conference on Machine learning. 2008: 1096-1103.</li>
<li>Rifai S, Vincent P, Muller X, et al. Contractive auto-encoders: Explicit invariance during feature extraction[C]//Icml. 2011.</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Auto-Encoder</tag>
        <tag>李宏毅</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习作业</title>
    <url>/lhy-exercise.html</url>
    <content><![CDATA[<p>记录李宏毅机器学习课程的作业。</p>
<span id="more"></span>

<p>需要安装的依赖包及对应的版本：</p>
<figure class="highlight text"><figcaption><span>requirements.txt</span></figcaption><table><tr><td class="code"><pre><span class="line">numpy==1.19.5</span><br><span class="line">pandas==1.1.5</span><br><span class="line">matplotlib==3.3.4</span><br><span class="line">tensorflow-cpu==2.4.0</span><br><span class="line">keras==2.4.3</span><br></pre></td></tr></table></figure>
<p>安装命令：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt -i https://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习笔记——GAN</title>
    <url>/lhy-gan.html</url>
    <content><![CDATA[<p>GAN（Generative Adversarial Network）全称是对抗生成网络，可以用来生成一些东西，例如图片或者句子等。</p>
<p>GAN相关技术：<a href="https://github.com/hindupuravinash/the-gan-zoo">the-gan-zoo</a></p>
<span id="more"></span>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="GAN的概念"><a href="#GAN的概念" class="headerlink" title="GAN的概念"></a>GAN的概念</h2><p>GAN有两部分组成：<strong>Generator</strong>（生成器）和 <strong>Discriminator</strong>（判别器）。</p>
<p>Generator是一个神经网络，它的输入是一个向量，输入向量的每个维度代表一些特征，输出一个图片或者一个句子（高维向量）。</p>
<p>Discriminator也是一个神经网络，它的输入是Generator生成的东西（一张图片或者一个句子），输出是一个Scalar（数值），这个数值代表产生出来的图片或者句子的真实性。即Discriminator是用来判断生成的东西是不是机器生成的。</p>
<p>GAN的<strong>基本思想</strong>：Generator用来生成图片，Discriminator用来判断这个图片是不是机器生成的，如果Discriminator无法对Generator生成的图片进行判别，Generator就是一个比较好的生成器。Generator和Discriminator的关系就是对抗的（adversarial）关系，他们在学习的过程中会一起进步。</p>
<p>GAN的<strong>训练方法</strong>：</p>
<ol>
<li>初始化 generator(G) 和 discriminator(D)。</li>
<li>在每次迭代训练过程中<ol>
<li>固定 G 的参数，更新 D 的参数。<br> 随机采样一些向量输入到 G，让G生成一些图片；然后使用 G 生成的图片和真实的图片训练 D，训练 D 的时候，如果图片来自 G，则输出较低的分数，如果图片是真实的，则输出较高的分数。</li>
<li>固定 D 的参数，更新 G 的参数。<br> 调整 G 的参数，使得它产生的图片能让 D 打出高分，也就是 G 要学习怎么“骗过” D。</li>
</ol>
</li>
</ol>
<blockquote>
<p>随机采样一些向量时，这些向量可以是一个符符合高斯分布的随机向量。</p>
</blockquote>
<p>练习：动漫头像生成器。</p>
<h2 id="结构化学习"><a href="#结构化学习" class="headerlink" title="结构化学习"></a>结构化学习</h2><p>GAN可以被看作一种结构化学习（Structured Learning）技术。</p>
<blockquote>
<p>机器学习可以看作是找到一个这样的函数f：$f:x \rightarrow y$，给定一个输入x，输出一个y。<br>回归问题：输出是一直数值（scalar）。<br>分类问题：输出一个类（one-hot 向量）。<br>结构化学习/预测：输出是一个序列、矩阵、图片、树…。</p>
</blockquote>
<p>结构化学习具体应用：</p>
<ol>
<li>输出序列：机器翻译、语音识别、聊天机器人等。</li>
<li>输出矩阵：根据图片画图$^{[1]}$、根据描述画图$^{[2]}$等。</li>
</ol>
<p>结构化学习的<strong>挑战</strong>：</p>
<ul>
<li>One-shot/Zero-shot Learning。有些类别只有一个样本或者没有样本。<br>  在分类问题中，每个类别都要有一些样本。但是在结构化学习中，每个可能的输出都可以看作一个“类”，由于输出空间是巨大的，大部分“类”可能没有训练数据，机器必须创造一些新的东西。</li>
<li>机器必须学习去做规划<br>  机器生成对象的时候是一部分一部分生成的，但是它应该对生成对象的结构有一个大致的了解，因为输出的各个部分是由依赖关系的，必须从全局去考虑。</li>
</ul>
<p>结构化学习<strong>方法</strong>：</p>
<ul>
<li>自底向上（Bottom Up）。先生成对象的每个部分，再组合到一起。Generator采用的方法。</li>
<li>自顶向下（Top Down）。直接生成对象，然后找到其中最好的一个。Discriminator采用的方法。</li>
</ul>
<h2 id="Genarator自己学习"><a href="#Genarator自己学习" class="headerlink" title="Genarator自己学习"></a>Genarator自己学习</h2><p>给Genarator输入不同的向量，Genarator输出不同的对象。</p>
<p>怎么产生输入向量？</p>
<ol>
<li>随机生成一些向量。</li>
<li>使用Auto-encoder，根据图片或者句子产生向量。 Variational Auto-encoder(VAE)，VAE具有降噪功能。</li>
</ol>
<p>待生成对象各个部分之间的关系是很重要的，但是它们不能互相影响。同时使用AE和GAN生成对象的话，AE往往需要更深的网络结构才能得到跟GAN相近的结果。</p>
<p>示例：数字生成器。</p>
<h2 id="Discriminator生成对象"><a href="#Discriminator生成对象" class="headerlink" title="Discriminator生成对象"></a>Discriminator生成对象</h2><h1 id="使用GAN生成序列"><a href="#使用GAN生成序列" class="headerlink" title="使用GAN生成序列"></a>使用GAN生成序列</h1><h2 id="Conditional-Sequence-Generation"><a href="#Conditional-Sequence-Generation" class="headerlink" title="Conditional Sequence Generation"></a>Conditional Sequence Generation</h2><p>产生序列的问题都可以看作是Conditional Sequence Generation。例如：</p>
<ul>
<li>语音辨识。Generator 的输入是一段声音信号，输出是这段声音信号对应的文字。</li>
<li>机器翻译。中翻英问题中，Generator 的输入是中文，输出是翻译过的英文序列。</li>
<li>聊天机器人。Generator 输入是一个句子，输出是另外一个句子。</li>
</ul>
<p>以上任务中的Generator都是一个seq2seq模型。</p>
<h3 id="RL-human-feedback"><a href="#RL-human-feedback" class="headerlink" title="RL(human feedback)"></a>RL(human feedback)</h3><h3 id="GAN-discriminator-feedback"><a href="#GAN-discriminator-feedback" class="headerlink" title="GAN(discriminator feedback)"></a>GAN(discriminator feedback)</h3><h2 id="Unsupervised-Conditional-Sequence-Generation"><a href="#Unsupervised-Conditional-Sequence-Generation" class="headerlink" title="Unsupervised Conditional Sequence Generation"></a>Unsupervised Conditional Sequence Generation</h2><h3 id="Text-Style-Transfer"><a href="#Text-Style-Transfer" class="headerlink" title="Text Style Transfer"></a>Text Style Transfer</h3><h3 id="Unsupervised-Abstractive-Summarization"><a href="#Unsupervised-Abstractive-Summarization" class="headerlink" title="Unsupervised Abstractive Summarization"></a>Unsupervised Abstractive Summarization</h3><h3 id="Unsupervised-Translation"><a href="#Unsupervised-Translation" class="headerlink" title="Unsupervised Translation"></a>Unsupervised Translation</h3><hr>
<p><strong>【参考资料】</strong></p>
<ol>
<li>Isola P, Zhu J Y, Zhou T, et al. Image-to-image translation with conditional adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1125-1134.</li>
<li>Reed S, Akata Z, Yan X, et al. Generative adversarial text to image synthesis[C]//International Conference on Machine Learning. PMLR, 2016: 1060-1069.</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习笔记——keras</title>
    <url>/lhy-keras.html</url>
    <content><![CDATA[<p>介绍使用keras建立神经网络的步骤。</p>
<p>keras版本：2.0</p>
<span id="more"></span>

<h1 id="keras建立神经网络"><a href="#keras建立神经网络" class="headerlink" title="keras建立神经网络"></a>keras建立神经网络</h1><h2 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h2><p>以全连接网络的手写数字识别为例。输入是维度为28*28的图片，输出是图片对应每个的数字的概率。<br><img src="data:image/png;base64,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" data-original="./nn_dense.jpg"><br>使用keras建立模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 使用add()添加层。</span></span><br><span class="line"><span class="comment"># Dense：全连接层。</span></span><br><span class="line"><span class="comment"># input_dim：输入维度。units：神经元个数。activation：激活函数。</span></span><br><span class="line">model.add(Dense(input_dim=<span class="number">28</span>*<span class="number">28</span>, units=<span class="number">500</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment"># 注意：只有模型的第一层需要指定input_dim，第2层的输入维度就是上一层的units。</span></span><br><span class="line">model.add(Dense(units=<span class="number">500</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment"># 由于输出是10个数字，因此最后一层的units必须是10。</span></span><br><span class="line">model.add(Dense(units=<span class="number">500</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure>

<h2 id="配置模型"><a href="#配置模型" class="headerlink" title="配置模型"></a>配置模型</h2><p>定义loss函数，选择优化器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>可选优化器：SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam</p>
<h2 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h2><p>使用梯度下降训练网络。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">100</span>, epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<h2 id="保存和加载模型"><a href="#保存和加载模型" class="headerlink" title="保存和加载模型"></a>保存和加载模型</h2><p><a href="https://keras.io/api/models/model_saving_apis/">Model saving &amp; serialization APIs</a></p>
<h2 id="使用模型"><a href="#使用模型" class="headerlink" title="使用模型"></a>使用模型</h2><ol>
<li>检查模型在测试集上的表现<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">score = model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure></li>
<li>用模型做预测<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.predict(x_test)</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>keras</tag>
        <tag>李宏毅</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习笔记——Regression</title>
    <url>/lhy-regression.html</url>
    <content><![CDATA[<p>机器学习要做的就是找一个函数，Regression要做的就是找一个输出是数字（Scalar）的函数。例子：</p>
<ul>
<li>股票市场预测。输入是过去一段时间股票市场变动的情况，输出是明天股票的数值。</li>
<li>自动驾驶。输入是无人车上各个传感器的数值，输出是方向盘的角度等。</li>
<li>商品推荐。输入是使用者的购买记录、商品的特性等，输出是某个使用者购买某些商品的可能性。<span id="more"></span></li>
</ul>
<p>Regression过程：</p>
<ol>
<li>建立模型<br> 线性模型：$y = b + \sum w_ix_i$</li>
<li>衡量模型好坏：Loss Function（L）<br> Loss函数输入是一个函数，输出是这个函数的好坏。$L(f) = \sum^n_{i=1} (\hat{y^i} - f(x^i))^2$。</li>
<li>找一个最好的函数（Optimal问题）：最小化损失函数<br> 找到一组使得损失函数值最低的参数。通常的方法：Gradient Descent。</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习笔记——RNN</title>
    <url>/lhy-rnn.html</url>
    <content><![CDATA[<p>介绍RNN和LSTM的基本概念和运作过程。</p>
<span id="more"></span>
<h1 id="RNN及其变体"><a href="#RNN及其变体" class="headerlink" title="RNN及其变体"></a>RNN及其变体</h1><h2 id="Recurrent-Neural-Network-RNN-介绍"><a href="#Recurrent-Neural-Network-RNN-介绍" class="headerlink" title="Recurrent Neural Network(RNN)介绍"></a>Recurrent Neural Network(RNN)介绍</h2><h3 id="RNN基本概念"><a href="#RNN基本概念" class="headerlink" title="RNN基本概念"></a>RNN基本概念</h3><p>RNN是带有<strong>记忆单元</strong>的前馈神经网络。在RNN里面，隐藏层的每次输出都会被保存到memory里面，下一次有输入时，隐藏层除了考虑输入层的数据，还会考虑memory里面的值。<br><img src="data:image/png;base64,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" data-original="./rnn_arch.jpg" alt="RNN网络结构"><br><strong>RNN只能记得前一个时间点的数据。</strong></p>
<h3 id="RNN工作过程"><a href="#RNN工作过程" class="headerlink" title="RNN工作过程"></a>RNN工作过程</h3><p>假设所有的weight都是1，bias为0，所有的激活函数都是线性函数。<br>input sequence：[[1, 1], [1, 1], [2, 2]…]<br><img src="data:image/png;base64,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" data-original="rnn_exp.jpg" alt="rnn example"></p>
<ol>
<li>初始化memory。memory初始化为[0, 0]。</li>
<li>从input sequence按顺序取一个值输入网络。输入[1, 1]。</li>
<li>计算隐藏层输入值。需要同时考虑输入层和memory中数据，隐藏层输出[2, 2]。</li>
<li>计算网络输出。只考虑上一个隐藏层的值，输出[4, 4]。</li>
<li>隐藏层中值存入memory。</li>
<li>重复2-4，直到input sequence中数据全部处理完成。</li>
</ol>
<h3 id="深层RNN"><a href="#深层RNN" class="headerlink" title="深层RNN"></a>深层RNN</h3><p>RNN的hidden layer可以有多层。下图中绿色方框表示hidden layer。<br><img src="data:image/png;base64,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" data-original="./Depp_rnn.jpg" alt="深层RNN"><br>RNN有不同的变形：</p>
<ul>
<li>Elan Network。memory中保存hidden layer的值。</li>
<li>Jordan Network。memory中保存<strong>output layer</strong>的值。</li>
</ul>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="./Elman_Jordan.jpg" alt="Elan Network "></p>
<h3 id="Bidirectional-RNN（双向RNN）"><a href="#Bidirectional-RNN（双向RNN）" class="headerlink" title="Bidirectional RNN（双向RNN）"></a>Bidirectional RNN（双向RNN）</h3><p>RNN可以是双向的，RNN可以同时从左往右和从右往左读取sequence，然后把同一时刻hidden layer的接给output layer。<br><img src="data:image/png;base64,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" data-original="./bidirectional_rnn.jpg" alt="Bidirectional RNN"><br>用bidirectional RNN的好处：网络在产生输出时看到的信息的范围比较广。</p>
<h2 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory(LSTM)"></a>Long Short-term Memory(LSTM)</h2><h3 id="LSTM基本概念"><a href="#LSTM基本概念" class="headerlink" title="LSTM基本概念"></a>LSTM基本概念</h3><p>LSTM有3个<code>Gate</code>：</p>
<ul>
<li><code>Input Gate</code>。 当某个神经元的输出想要写入memory cell时必须通过一个<code>Input Gate</code>，<code>Input Gate</code>是打开状态时其他神经元的值才能被写入进来。</li>
<li><code>Output Gate</code>。memory cell输出时有一个<code>Output Gate</code>，<code>Output Gate</code>是打开状态时其他神经元才能从memory cell中把值读出来。</li>
<li><code>Forget Gate</code>。决定memory cell什么时候应该把过去记得的信息忘掉。<code>Forget Gate</code><strong>打开表示记得，关闭表示遗忘</strong>。</li>
</ul>
<p>这3个<code>Gate</code>的状态都是神经元可以自己去学习到的。<br><img src="data:image/png;base64,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" data-original="./lstm_cell.jpg" alt="lstm cell"><br>LSTM神经元是一个特殊的神经元，可以看作有<strong>4个input</strong>和1个output。<strong>4个input的值是通过来自其它神经元的值乘上相应的权重得到的</strong>，因此使用lstm神经网络的<strong>参数量</strong>将是其他普通神经网络的<strong>4倍</strong>。</p>
<h3 id="LSTM工作过程"><a href="#LSTM工作过程" class="headerlink" title="LSTM工作过程"></a>LSTM工作过程</h3><p>$z$：其他神经元的输入。$z_i$：控制<code>Input Gate</code>状态。$z_o$：控制<code>Output Gate</code>状态。$z_f$：控制<code>Forget Gate</code>状态。$a$：最终的输出。$c$：memory cell中的原始值。<br><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="./lstm_detail.jpg" alt="lstm详解"></p>
<ol>
<li>$z$通过一个激活函数得到$g(z)$，$z_i$通过一个激活函数得到$f(z_i)$，$z_f$通过一个激活函数得到$f(z_f)$。</li>
<li>计算memory cell中的新值$c’$： $c’ = g(z)f(z_i) + cf(z_f)$。从这个式子可以看出，$f(z_i)$用来控制要不要把$g(z)$存到memory，$f(z_f)$用来决定要不要把原来存在memory中的值忘掉。</li>
<li> $c’$通过一个激活函数得到$h(c’)$，$z_o$通过一个激活函数得到$f(z_o)$。</li>
<li> 计算最终输出。$a = h(c’)f(z_o)$。</li>
</ol>
<p><em>3个<code>Gate</code>信号的激活函数<code>f</code>通常使用<code>sigmoid</code>函数。</em><br><strong>Example：</strong><br>输入是3维向量，输出是一维向量。<br>假设：</p>
<ol>
<li>权重如下图右。可知输入全为0时，<code>Input Gate</code>和<code>Output Gate</code>是关闭的，<code>Forget Gate</code>是打开的。即$x_2$的值为1时，$x_1$会被写入memory；$x_2$的值为-1时，memory中的值会被遗忘；$x_3$的值为1时，memory中的值才会输出。</li>
<li>memory初始值为0。</li>
<li>$g$和$h$是线性函数。<br><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="./lstm_exp.jpg" alt="lstm example"></li>
</ol>
<h3 id="与RNN的关系"><a href="#与RNN的关系" class="headerlink" title="与RNN的关系"></a>与RNN的关系</h3><p><img src="data:image/png;base64,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" data-original="./lstm_rnn.jpg" alt="与RNN的关系"><br>上图图左是简单版本的LSTM工作过程，其中$c_t$表示t时刻LSTM cell的记忆单元（memory cell）中的值。</p>
<p>图右是LSTM实际的工作过程，图中$h_t$表示t时刻hidden layer层的值，即t时刻LSTM cell的输出值$y_t$。LSTM每个时刻的输入除了当前时刻的输入数据（$x_t$），还考虑了<strong>前一个时刻</strong>记忆单元中的值（$c_{t-1}$和）和隐藏层的输出（$h_{t-1}$）。</p>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><h1 id="RNN网络结构"><a href="#RNN网络结构" class="headerlink" title="RNN网络结构"></a>RNN网络结构</h1><p><img src="data:image/png;base64,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" data-original="./rnn_struct.jpg" alt="many to many"></p>
<h2 id="many-to-one"><a href="#many-to-one" class="headerlink" title="many to one"></a>many to one</h2><p>输入多个向量（sequence），输入一个向量。①<br>应用场景：语义分析、关键词提取</p>
<h2 id="many-to-many"><a href="#many-to-many" class="headerlink" title="many to many"></a>many to many</h2><ol>
<li>输入和输出都是sequence，但是<strong>输出更短</strong>。②<br> 使用的技巧：CTC(Connectionist Temporal Classification)<br> 应用场景：语音识别$^{[1]}$<br> <img src="data:image/png;base64,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" data-original="./many_to_many1.jpg" alt="语音识别"></li>
<li>输入和输出都是sequence，<strong>长度不相同</strong>。③<br> 应用场景：机器翻译<br> <img src="data:image/png;base64,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" data-original="./many_to_many2.jpg" alt="机器翻译"></li>
</ol>
<h1 id="RNN的更多应用"><a href="#RNN的更多应用" class="headerlink" title="RNN的更多应用"></a>RNN的更多应用</h1><h2 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a>文本生成</h2><p>句子可以看作由字符或者单词组成，可以使用RNN每次生成一个字符或单词。</p>
<p>例子：使用RNN写诗。<br><img src="data:image/png;base64,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" data-original="generation.jpg" alt="generation"></p>
<ol>
<li>给定一个开始字符<code>&lt;BOS&gt;</code>：begin of sentence。</li>
<li>生成第一个token，$y^1$：P(w|<code>&lt;BOS&gt;</code>&gt;)。句子中上一个token（字符或者单词）是$x^1$时，下一个可能的token的分布。从$y^1$中选择token的方法有两种：sample和argmax。</li>
<li>生成第二个token，$y^2$：P(w|<code>&lt;BOS&gt;</code>，床)。<u>问题：$h^1$中不会保存“床”的信息吗？</u></li>
<li>继续生成token，直到遇到结束字符<code>&lt;EOS&gt;</code>。</li>
</ol>
<h2 id="文法解析（Syntactic-parsing）"><a href="#文法解析（Syntactic-parsing）" class="headerlink" title="文法解析（Syntactic parsing）"></a>文法解析（Syntactic parsing）</h2><p>将文法解析树用括号表示法表述出来，可以将树看作一个序列$^{[2]}$。<br><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="./Syntactic_parsing.jpg" alt="Syntactic parsing"></p>
<hr>
<p><strong>【参考资料】</strong></p>
<ol>
<li> Graves A, Jaitly N. Towards end-to-end speech recognition with recurrent neural networks[C]//International conference on machine learning. PMLR, 2014: 1764-1772.</li>
<li> Vinyals O, Kaiser Ł, Koo T, et al. Grammar as a foreign language[J]. Advances in neural information processing systems, 2015, 28: 2773-2781.</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习笔记——自注意力机制</title>
    <url>/lhy-self-attention.html</url>
    <content><![CDATA[<p>self-attention也是一个常见的neural network框架，用来解决输入是一组向量并且向量的个数是不确定的问题。<span id="more"></span></p>
<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><ul>
<li>文本处理</li>
<li>语音识别</li>
<li>Drug discovery</li>
</ul>
<p>self-attention的输出有以下3种情况：</p>
<ol>
<li><strong>每个向量都有一个标签</strong>，即输出跟输入数目一样。具体应用：词性标注、推荐系统。</li>
<li><strong>整个sequence只输出一个标签</strong>。具体应用：语义分析、说话人识别、图片（分子结构图）识别。</li>
<li><strong>输出的标签个数不确定</strong>。具体应用：机器翻译。</li>
</ol>
<h1 id="为什么要使用Self-attention"><a href="#为什么要使用Self-attention" class="headerlink" title="为什么要使用Self-attention?"></a>为什么要使用Self-attention?</h1><p>对于输出与输入数目相同的情况，这种问题也称作Sequence Labeling问题。例如对句子<code>I saw a saw</code>中的单词进行词性标注，在这句话中，第一个<code>saw</code>是动词，第二个<code>saw</code>是名词。如果使用全连接网络来处理，有两种方法：</p>
<ol>
<li><strong>逐个向量处理</strong>。这种方法的缺陷是，它无法利用序列的上下文信息，全连接网络对于任意位置<code>saw</code>的词性预测结果必然是一样的。</li>
<li><strong>考虑上下文信息</strong>，即在对<code>saw</code>进行词性标注时，设置一个window同时将前面几个单词和后面几个单词考虑进来。但是由于句子长度的不确定性，window设置过小，则不能将长句子中的所有单词考虑进来；window设置过大，又会使得网络参数太多，导致过拟合。</li>
</ol>
<p>为了将整个句子的信息考虑进来，就要用到self-attention技术。self-attention网络结构：<br><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original=".%5Cself-attention.jpg#pic_center" alt="self-attention"><br>self-attention对于每个向量都会考虑整个sequence的信息后输出一个向量。</p>
<h1 id="Self-attention计算过程"><a href="#Self-attention计算过程" class="headerlink" title="Self-attention计算过程"></a>Self-attention计算过程</h1><blockquote>
<p>计算两个向量相关性的方法：</p>
<ol>
<li><strong>点乘运算（dot product）</strong>。最常用的方法。输入向量分别乘上两个不同的矩阵 $W_{q}$ 和 $W_{k}$ 得到向量 $q$ 和 $k$ ，再把 $q$ 和 $k$ 做点乘。 $a_{i}$ 和  $a_{j}$ 的相关度 $\alpha_{i,j} = (a_{i}W_{q}) . (a_{j}W_{k})$ 。</li>
<li>加性运算（additive）。不对 $q$ 和 $k$ 做点乘，而是串联起来后使用 $tanh$ 函数激活。$a_{i}$ 和  $a_{j}$ 的相关度 $\alpha_{i,j} = tanh((a_{i}W_{q}) + (a_{j}W_{k}))$ 。</li>
</ol>
</blockquote>
<p>在self-attention中，计算attention的步骤：<br><img src="data:image/png;base64,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" data-original="./attention_compute.jpg#pic_center" alt="attention计算步骤"></p>
<ol>
<li>计算 $query$，即当前向量与 $W_{q}$ 的乘积。计算方法 ：$q_{i} = W_{q}a_{i}$。</li>
<li>计算 $key$，即其他向量与 $W_{k}$ 的乘积 。计算方法：$k_{j} = W_{k}a_{j},  k\in[1, n]$。</li>
<li>计算attetion score ($\alpha$)，即当前向量与其他向量的相关性。计算方法：$\alpha_{i, j} = q_{i}.k_{j}$。<em>通常情况下，也需要计算向量和自己的相关性</em> 。</li>
<li>计算$\alpha’$。使用softmax函数激活$\alpha$得到$\alpha’$（也可以使用其他激活函数）。$\alpha_{i, j}’ = softmax(\alpha_{i, j})$。</li>
<li>计算 $value$ 。根据$\alpha’$抽取sequence中重要信息。计算方法 ：$v_{j} = W_{v}a_{j}$</li>
<li>计算输出向量$b_{i}$。计算方法 ：$b_{i} =\sum_{j=1}^n\alpha_{i, j}’.v_{j}$。</li>
</ol>
<p><mark>self-attention中输出向量是同时计算出来的，不需要按序计算。</mark><br>以上步骤使用矩阵操作的方式可以简单描述为：</p>
<ol>
<li>根据输入向量<code>I</code>计算 <code>Q、K、V</code>。</li>
<li>计算分数。</li>
<li>计算输出向量<code>O</code>。</li>
</ol>
<p><img src="data:image/png;base64,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" data-original="./attention_vector.jpg#pic_center" alt="在这里插入图片描述"><br>从这个计算过程可以看出，self-attention需要训练的参数只有 $W_{q}、W_{k}和W_{v}$。</p>
<h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>到目前为止，self-attention中没有位置信息，它不知道每个向量在sequence中的位置是什么，也不知道两个向量间的距离是多少，但是<strong>位置信息有时候也是比较重要</strong>的。例如，在词性标注问题中，也许动词在句首出现的可能性比较低。<br>因此，当你认为你处理的问题中，位置信息比较重要，就需要用到<code>positional encoding</code>技术，也就是为每一个输入向量 $a_i$ 加上一个代表位置的向量 $e_i$ ，告诉self-attention位置的信息。<br>位置变量的设计：</p>
<ol>
<li>人工设计（hand-crafted）。</li>
<li>从数据中学习（研究中）。</li>
</ol>
<p><strong>【相关论文】</strong>  </p>
<ol>
<li>位置变量的设计方法：<a href="https://arxiv.org/abs/2003.09229">Learning to encode position for transformer with continuous dynamical model_2003</a></li>
</ol>
<h1 id="Self-attention的变体"><a href="#Self-attention的变体" class="headerlink" title="Self-attention的变体"></a>Self-attention的变体</h1><h2 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head Self-attention"></a>Multi-head Self-attention</h2><p>在要解决的实际问题中，计算相关性时可能会需要计算多个种类的相关性，这个时候就需要设置多个<code>q</code>，每个<code>q</code>负责一个种类的相关性，相应的<code>k、v</code>也需要多个。这里<code>q、k、v</code>的个数即<code>head</code>的个数。<br>attention计算过程：</p>
<ol>
<li>计算<code>Q、K、V</code>后，分别乘上另外<code>n</code>（n表示head个数）个矩阵，得到 $Q_{1, n}、K_{1, n}、V_{1, n}$。</li>
<li>计算分数和正则化。对每个<code>Q、K、V</code>的计算过程与self-attention相同。</li>
<li>计算输出。将得到的<code>n</code>个<code>O</code>乘以向量$W_o$得到最终输出。</li>
</ol>
<h2 id="Truncated-Self-attention"><a href="#Truncated-Self-attention" class="headerlink" title="Truncated Self-attention"></a>Truncated Self-attention</h2><p>适用于计算<code>attention</code>时不需要考虑整个sequence，只需要考虑sequence中的一个小范围的问题。这个时候使用<code>Truncated Self-attention</code>可以加快运算速度。</p>
<h2 id="Masked-Self-attention"><a href="#Masked-Self-attention" class="headerlink" title="Masked Self-attention"></a>Masked Self-attention</h2><p>对于输出的每个向量，只考虑当前位置之前的输入向量，而不是考虑全部的输入向量。</p>
<p>为什么需要masked？参考Transformer Decoder的工作过程（把当前位置输出作为下一个位置的输出），这种情况下在计算当前位置输出的时候，无法获取之后的输入。</p>
<p><strong>【相关论文】</strong></p>
<ol>
<li>self-attention各种变体的比较：<a href="https://arxiv.org/abs/2011.04006">Long Range Arena: A Benchmark for Efficient Transformers_2011</a></li>
<li>self-attention各种变体的介绍：<a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a></li>
</ol>
<h1 id="Self-attention与其他神经网络的比较"><a href="#Self-attention与其他神经网络的比较" class="headerlink" title="Self-attention与其他神经网络的比较"></a>Self-attention与其他神经网络的比较</h1><h2 id="Self-attention-v-s-CNN"><a href="#Self-attention-v-s-CNN" class="headerlink" title="Self-attention v.s. CNN"></a>Self-attention v.s. CNN</h2><p>CNN的每个神经元只考虑一个感受野（receptive field）中的信息，Self-attention考虑的整张图片的信息。</p>
<ul>
<li>CNN是简化版的Self-attention。</li>
<li>Self-attention是复杂化的CNN。</li>
</ul>
<p><strong>【相关论文】</strong></p>
<ol>
<li>Self-attention与CNN的关系： <a href="https://arxiv.org/abs/1911.03584">On the Relationship between Self-Attention and Convolutional Layers_1911</a></li>
<li>Self-attention与CNN模型的比较：<a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale_2010</a></li>
</ol>
<h2 id="Self-attention-v-s-RNN"><a href="#Self-attention-v-s-RNN" class="headerlink" title="Self-attention v.s. RNN"></a>Self-attention v.s. RNN</h2><p>不同点：</p>
<ol>
<li>RNN在输出一个向量时需要考虑的向量必须在memory(记忆)中，Self-attention只需要一个<code>q</code>和一个<code>k</code>就可以了。</li>
<li>RNN在处理输入的多个向量时不能并行处理，Self-attention可以并行处理所有输出。</li>
</ol>
<p><strong>【相关论文】</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/2006.16236">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention_2006</a></li>
</ol>
<h2 id="Self-attention-v-s-GNN"><a href="#Self-attention-v-s-GNN" class="headerlink" title="Self-attention v.s. GNN"></a>Self-attention v.s. GNN</h2><p>to do…</p>
<h1 id="Self-attention的其他应用"><a href="#Self-attention的其他应用" class="headerlink" title="Self-attention的其他应用"></a>Self-attention的其他应用</h1><h2 id="图像（image）处理"><a href="#图像（image）处理" class="headerlink" title="图像（image）处理"></a>图像（image）处理</h2><p><strong>【相关论文】</strong></p>
<ol>
<li>Self-attention GAN。<a href="https://arxiv.org/abs/1805.08318">Self-Attention Generative Adversarial Networks_1805 </a></li>
<li>DEtection Transformer(DETR)。<a href="https://arxiv.org/abs/2005.12872">End-to-End Object Detection with Transformers_2005 </a></li>
</ol>
<p>比较self-attention与CNN</p>
<h2 id="图（graph）结构数据处理"><a href="#图（graph）结构数据处理" class="headerlink" title="图（graph）结构数据处理"></a>图（graph）结构数据处理</h2><p>比较self-attention与GNN</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>self-attention</tag>
        <tag>自注意力</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习笔记_seq2seq</title>
    <url>/lhy-seq2seq.html</url>
    <content><![CDATA[<p>使用神经网络生成有结构的对象$^{[1]}$。例如：</p>
<ul>
<li>生成句子。使用RNN每次生成一个字符/单词。</li>
<li>生成图片。使用RNN每次生成一个像素。<span id="more"></span>
<img src="data:image/png;base64,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" data-original="./generation.jpg" alt="generation"></li>
</ul>
<h1 id="Conditional-Generation"><a href="#Conditional-Generation" class="headerlink" title="Conditional Generation"></a>Conditional Generation</h1><p>如果只是使用RNN，就只能简单的生成一些随机的句子，但是在有些场景下需要根据一些条件生成句子。例如：</p>
<ul>
<li>Image Caption Generation。生成图片标题。根据图片内容生成句子。</li>
<li>Chat-bot$^{[2]}$。</li>
<li>Machine translation。</li>
</ul>
<p>做法：</p>
<ol>
<li>把输入条件表示成一个向量。这个步骤称作 <strong>Encoder</strong>。</li>
<li>然后把这个向量作为RNN生成器的输入。这个步骤称作 <strong>decoder</strong>。<br><img src="data:image/png;base64,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" data-original="./conditional_generation.jpg" alt="conditional generation"><br>像这样，把Encoder和Decoder联合起来训练就是sequence to sequence learning。</li>
</ol>
<h1 id="Attention（Dynamic-Conditional-Generation）"><a href="#Attention（Dynamic-Conditional-Generation）" class="headerlink" title="Attention（Dynamic Conditional Generation）"></a>Attention（Dynamic Conditional Generation）</h1><p>原来Decoder的输入是整个句子的信息，但是如果Encoder的输入信息很复杂，可能没法用一个向量表示，这时由于Decoder每次输入都是相同的（整个句子的信息），就会导致结果没有那么好，而Attention可以让Decoder只看到比较需要的部分信息，即<strong>每次输入不同的信息</strong>。</p>
<p>应用场景：</p>
<ul>
<li>机器翻译。</li>
<li>语音识别$^{[3]}$（Speech Recognition）。</li>
<li>生成图片标题$^{[4]}$（Image Caption Generation）。</li>
<li>生成视频描述$^{[5]}$。</li>
</ul>
<p>以机器翻译为例：<br><img src="data:image/png;base64,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" data-original="./attention_machine_translation.jpg" alt="machinetranslation"><br>$\alpha$：两个向量的相关度。$\alpha^1_0$表示第一个隐藏层（RNN）输出与$z^0$的相关度。<br>$z^1$：可以取自RNN隐藏层的输出。</p>
<h2 id="Memory-Network-6-。"><a href="#Memory-Network-6-。" class="headerlink" title="Memory Network$^{[6]}$。"></a>Memory Network$^{[6]}$。</h2><p>在memory上用attention，最开始被应用于reading comprehension上。</p>
<h2 id="Neural-Turing-Machine。"><a href="#Neural-Turing-Machine。" class="headerlink" title="Neural Turing Machine。"></a>Neural Turing Machine。</h2><p>通过attention将信息写入memory。</p>
<h1 id="生成句子的技巧"><a href="#生成句子的技巧" class="headerlink" title="生成句子的技巧"></a>生成句子的技巧</h1><ol>
<li>使用Attention时，<em>好的attention</em>是：输入的每一个部分应该获得几乎相同的关注度$^{[4]}$。</li>
<li>处理generation过程训练和测试中的mismatch的方法<ol>
<li>修改训练过程，修改成与测试过程类似（使用上一步输出值作为当前输入）。不可行。</li>
<li>Scheduled Sampling$^{[7]}$。随机选择上一步输出和真实值作为下一步输入。</li>
</ol>
</li>
</ol>
<h1 id="实现英译汉"><a href="#实现英译汉" class="headerlink" title="实现英译汉"></a>实现英译汉</h1><p>序列的顺序十分重要，因此seq2seq通常使用LSTM作为编码器和解码器。</p>
<p>使用attention和不使用attention的比较$^{[8]}$：</p>
<ul>
<li>没有attention机制的encoder-decoder结构通常把encoder的最后一个状态作为decoder的输入（可能作为初始化，也可能作为每一时刻的输入），但是encoder的state毕竟是有限的，存储不了太多的信息，对于decoder过程，每一个步骤都和之前的输入都没有关系了，只与这个传入的state有关。</li>
<li>attention机制的引入之后，decoder根据时刻的不同，让每一时刻的输入都有所不同。</li>
</ul>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ol>
<li>对英文进行特征提取<br> 将英文按顺序输入到LSTM中，LSTM会对英文进行特征提取。</li>
<li>将提取到的特征传入到decoder</li>
</ol>
<p><strong>【参考资料】</strong><br>视频：<a href="https://www.bilibili.com/video/BV1it4y1X7Qd?p=40">seq2seq</a></p>
<ol>
<li>使用RNN生成有结构的对象相关论文<ul>
<li>Image<ul>
<li>Van Oord A, Kalchbrenner N, Kavukcuoglu K. Pixel recurrent neural networks[C]//International Conference on Machine Learning. PMLR, 2016: 1747-1756.</li>
<li>Oord A, Kalchbrenner N, Vinyals O, et al. Conditional image generation with pixelcnn decoders[J]. arXiv preprint arXiv:1606.05328, 2016.</li>
</ul>
</li>
<li>Video<ul>
<li>Van Oord A, Kalchbrenner N, Kavukcuoglu K. Pixel recurrent neural networks[C]//International Conference on Machine Learning. PMLR, 2016: 1747-1756.</li>
</ul>
</li>
<li>Handwriting<ul>
<li>Graves A. Generating sequences with recurrent neural networks[J]. arXiv preprint arXiv:1308.0850, 2013.</li>
</ul>
</li>
<li>Speech<ul>
<li>Oord A, Dieleman S, Zen H, et al. Wavenet: A generative model for raw audio[J]. arXiv preprint arXiv:1609.03499, 2016.</li>
</ul>
</li>
</ul>
</li>
<li>Conditional Generation的在聊天机器人上的应用<br> Serban I, Sordoni A, Bengio Y, et al. Building end-to-end dialogue systems using generative hierarchical neural network models[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2016, 30(1).</li>
<li>Attention在的语音识别上应用<br> Chan W, Jaitly N, Le Q V, et al. Listen, attend and spell[J]. arXiv preprint arXiv:1508.01211, 2015.</li>
<li>Attention在生成图片标题的应用<br> Xu K, Ba J, Kiros R, et al. Show, attend and tell: Neural image caption generation with visual attention[C]//International conference on machine learning. PMLR, 2015: 2048-2057.</li>
<li>Attention在生成视频描述的应用<br> Yao L, Torabi A, Cho K, et al. Describing videos by exploiting temporal structure[C]//Proceedings of the IEEE international conference on computer vision. 2015: 4507-4515.</li>
<li>memory network<ul>
<li>Sukhbaatar S, Szlam A, Weston J, et al. End-to-end memory networks[J]. arXiv preprint arXiv:1503.08895, 2015.</li>
<li>Fang W, Hsu J Y, Lee H, et al. Hierarchical attention model for improved machine comprehension of spoken content[C]//2016 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2016: 232-238.</li>
</ul>
</li>
<li>Scheduled sampling<br> Bengio S, Vinyals O, Jaitly N, et al. Scheduled sampling for sequence prediction with recurrent neural networks[J]. arXiv preprint arXiv:1506.03099, 2015.</li>
<li><a href="https://blog.csdn.net/qq_35549634/article/details/106603346">tensorflow2.0(Keras)实现seq2seq+Attention模型的对话系统–实战篇（序列生成）</a></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习笔记——Transfer-learning</title>
    <url>/lhy-transfer-learning.html</url>
    <content><![CDATA[<p>迁移学习适用于没有与当前任务直接相关数据的情形。例如：</p>
<ul>
<li>语音识别问题中，想要做某种方言的语音识别，但是方言的数据比较难获取，而汉语、英语等的数据很容易获取到。</li>
<li>图像识别问题中，想要做医学方面的图像识别，但是医学方面的图像数据很难获取，而猫、狗之类的图片很好获取。</li>
</ul>
<span id="more"></span>
<h1 id="迁移学习的方法"><a href="#迁移学习的方法" class="headerlink" title="迁移学习的方法"></a>迁移学习的方法</h1><blockquote>
<p>源数据（source data）：跟当前任务没有直接关系的数据。<br>目标数据（target data）：跟当前任务直接相关的数据。</p>
</blockquote>
<h2 id="源数据有标签，目标数据有标签"><a href="#源数据有标签，目标数据有标签" class="headerlink" title="源数据有标签，目标数据有标签"></a>源数据<strong>有</strong>标签，目标数据<strong>有</strong>标签</h2><p>任务描述；</p>
<ul>
<li>目标数据：$(x^t, y^t)$，数据量很少</li>
<li>源数据：$(x^s, y^s)$，数据量很多</li>
</ul>
<ol>
<li>Fine-tuning<br>例子：语音辨识系统</li>
</ol>
<p>某个人的语音数据很少（目标数据），很多人的语音数据很多（源数据）。</p>
<p>做法：用源数据训练模型，用目标数据<strong>微调</strong>（fine-tune）模型</p>
<ul>
<li>问题：小心过拟合<ul>
<li>Conservative Training。</li>
<li>Layer Transfer$^{[1]}$。使用源数据训练好模型后，从中复制一些layer出来，然后用目标数据训练没有复制的layer。<ul>
<li>那些层应该被复制？语音（后几层）、Image（前几层）</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="2">
<li>Multitask Learning</li>
</ol>
<h2 id="源数据无标签，目标数据有标签"><a href="#源数据无标签，目标数据有标签" class="headerlink" title="源数据无标签，目标数据有标签"></a>源数据<strong>无</strong>标签，目标数据<strong>有</strong>标签</h2><h2 id="源数据有标签，目标数据无标签"><a href="#源数据有标签，目标数据无标签" class="headerlink" title="源数据有标签，目标数据无标签"></a>源数据<strong>有</strong>标签，目标数据<strong>无</strong>标签</h2><h2 id="源数据无标签，目标数据无标签"><a href="#源数据无标签，目标数据无标签" class="headerlink" title="源数据无标签，目标数据无标签"></a>源数据<strong>无</strong>标签，目标数据<strong>无</strong>标签</h2><hr>
<ol>
<li>Yosinski J, Clune J, Bengio Y, et al. How transferable are features in deep neural networks?[J]. arXiv preprint arXiv:1411.1792, 2014.</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Transfer-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习笔记——Transformer</title>
    <url>/lhy-transformer.html</url>
    <content><![CDATA[<p>Transformer是一个sequence-to-sequence模型，它的输入是一个序列，输出也是一个序列，输出序列的长度由模型自己决定。</p>
<span id="more"></span>
<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><ol>
<li>语音辨识（Speech Recognition）：语音转文字。</li>
<li>机器翻译（Machine Translation）。</li>
<li>语音翻译（Speech Translation）：将语音转成指定语言的文字。有些语言没有文字，例如：方言中某些字词。</li>
<li>聊天机器人。</li>
<li>问答问题（Question Answering）。输入为一个问题和一篇文章，输出为问题答案。例如：Q：文章的摘要是什么？A：摘要是…。</li>
</ol>
<p>Seq2seq模型也可以应用在不是明确的序列到序列问题上，例如：</p>
<ol>
<li>语义解析（Syntactic Parsing）<br> 语义解析问题中，输入是一句话，输出一个语法解析树，如果把树用嵌套括号表示法表示出来，那么这个问题也可以看作是一个Seq2seq问题<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。</li>
<li>多标签分类问题（Multi-label Classification）<br> Multi-label Classification和Multi-class Classification的区别：前者是指一个输入属于<strong>多个</strong>类别，后者是指从多个类别中选出输入对应的那<strong>一个</strong>类别<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>。</li>
<li>目标检测（Object Detection）<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></li>
</ol>
<h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><p>Seq2seq模型由编码器（Encoder）和解码器（Decoder）两部分组成<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>。<br><img src="data:image/png;base64,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" data-original="./encoder_decoder.jpg" alt="transformer结构"></p>
<h2 id="编码器工作原理"><a href="#编码器工作原理" class="headerlink" title="编码器工作原理"></a>编码器工作原理</h2><p>编码器对每个输入向量都会产生一个输出。这个过程使用RNN、CNN和Self-attention也可以完成。</p>
<p>Transformer原始论文<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>中的Encoder使用的是加入了残差连接（residual connection）和正则化（Layer Normalization<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>）的Self-attention。Encoder的其他设计方法参考<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup>。<br><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="./encoder.jpg" alt="编码器"><br>Add&amp;Norm：Residual + Layer Norm</p>
<blockquote>
<p>Layer Norm和Batch Norm的区别：Batch Norm是对不同样本（example）同一个维度（dimension）不同的特征（feature）计算均值（mean）和标准差(standard deviation)，Layer Norm是对同一个样本同一个特征的不同维度计算均值和标准差。</p>
</blockquote>
<h2 id="解码器工作原理"><a href="#解码器工作原理" class="headerlink" title="解码器工作原理"></a>解码器工作原理</h2><p>Decoder用来产生输出序列，输出序列的长度由Decoder决定。</p>
<p>Decoder有两种：Autoregressive(AT)和Non-autoregressive(NAT)。</p>
<h3 id="Autoregressive"><a href="#Autoregressive" class="headerlink" title="Autoregressive"></a>Autoregressive</h3><p>AT产生输出的过程：</p>
<ol>
<li>增加一个特殊的符号（special token）：BEGIN，对这个special token进行one-hot编码后输入到Decoder。</li>
<li>Decoder输出一个长度为vocabulary size的向量。这个向量中是每个词汇的概率值。</li>
<li>选取上一步中概率最大的词汇继续输入到Decoder。</li>
<li>重复2、3步…</li>
</ol>
<p>怎么决定AT输出的长度？</p>
<ul>
<li>AT应该自己去学习句子的长度。因此需要在词汇表中增加一个特殊符号：END，当Deocder输出为END，则停止。</li>
</ul>
<h3 id="Non-autoregressive"><a href="#Non-autoregressive" class="headerlink" title="Non-autoregressive"></a>Non-autoregressive</h3><p>NAT产生输出的过程：</p>
<ol>
<li>同时给Decoder同时输入多个BEGIN，Decoder就会一次产生多个输入组成一个句子。</li>
</ol>
<p>怎么决定NAT输出的长度？有两种做法：    </p>
<ul>
<li>做法一：先使用一个Classifier预测输出的长度（输入为NAT的输入，输出为NAT应该输出的序列的长度）；然后给Decoder输入预测出的长度个BEGIN。</li>
<li>做法二：给定一个特别长的输入（例如300个BEGIN），此时也会输出一个长度为300的序列，再从中截取出输出序列中END前面的部分。</li>
</ul>
<p>与AT比较：</p>
<ul>
<li>优点：并行化；输出长度可控。</li>
<li>缺点：性能没有AT好。原因：<strong>Multi-modality</strong>。</li>
</ul>
<h2 id="编码器与解码器的交互过程"><a href="#编码器与解码器的交互过程" class="headerlink" title="编码器与解码器的交互过程"></a>编码器与解码器的交互过程</h2><p>解码器中读取编码器输出的部分称为Cross Attention。Cross Attention中的$q$来自Decoder，$k$和$v$来自Encoder。<br><img src="data:image/png;base64,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" data-original="cross_attention.jpg" alt="Cross Attention"></p>
<ol>
<li>Encoder中输入一排向量，对应输出一排向量$a_1 a_2 a_3$。</li>
<li>Decoder中输入BEGIN，经过Masked Self-attention输出一个向量，对这个向量乘上一个矩阵得到$q$。</li>
<li>对$a_1 a_2 a_3$进行变换得到$k_1 k_2 k_3$和$v_1 v_2 v_3$。</li>
<li>使用$q$和$k$计算attention分数并正则化，得到$\alpha_1’ \alpha_2’ \alpha_3’$。</li>
<li>把$\alpha_1’ \alpha_2’ \alpha_3’$乘上$v_1 v_2 v_3$并加起来得到$v$。</li>
<li>然后把$v$通过一个FC做接下来的处理。</li>
</ol>
<p>在原始论文中，每层Decoder都获取的是Encoder最后一层的输出。也有人尝试不同的连接方式<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup>。</p>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><h2 id="训练Seq2seq模型的Tips"><a href="#训练Seq2seq模型的Tips" class="headerlink" title="训练Seq2seq模型的Tips"></a>训练Seq2seq模型的Tips</h2><ol>
<li><p>Copy Mechanism<br> 有些任务中Decoder不需要自己创造输出，可以从输入中复制一些信息过来。例如：</p>
<ul>
<li>聊天机器人。复制前面对话中的某些内容。<br>  <em>User： 你好，我是*<em>库洛洛</em></em><br>  Machine：<strong>库洛洛</strong>你好，很高兴认识你*  </li>
<li>摘要<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup>。生成文章摘要时可以从文章中复制一些词汇出来。</li>
</ul>
<blockquote>
<p>了解Pointer Net。</p>
</blockquote>
</li>
<li><p>Guided Attention<br> 在某些任务中，输入和输出要求是严格对齐的。</p>
<ul>
<li>语音辨识。对于每个语音都要有对应的文字输出。</li>
<li>语音合成。输入的每个字都要有对应的语音合成。</li>
</ul>
<blockquote>
<p>了解Monotonic Attention、Location-aware attention</p>
</blockquote>
</li>
<li><p>Beam Search<br> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="./beam_search.jpg" alt="Beam Search"><br> Beam Search在一些输出比较确定的任务中表现比较好，例如：语音辨识。但是在结果具有随机性，需要一定创造力的时候表现就比较差，例如：文本生成<sup id="fnref:10"><a href="#fn:10" rel="footnote">10</a></sup>、语音合成。</p>
</li>
</ol>
<h2 id="优化评估指标"><a href="#优化评估指标" class="headerlink" title="优化评估指标"></a>优化评估指标</h2><p>验证的时候时候用BLEU score评估（比较两个句子），训练的使用的是cross entropy，但是最小化cross entropy时BLEU score不一定是最小的。</p>
<blockquote>
<p> 为什么训练的时候不用BLEU Score？因为BLEU score不能做微分。</p>
</blockquote>
<p><strong>当不知道怎么优化的损失函数时，就使用reinforcement learning(RL)！</strong>把损失函数当作RL的reward，把Decoder当作是Agent<sup id="fnref:11"><a href="#fn:11" rel="footnote">11</a></sup>。</p>
<h2 id="mismatch问题"><a href="#mismatch问题" class="headerlink" title="mismatch问题"></a>mismatch问题</h2><p>训练Decoder时，给Decoder的输入为Ground Truth（正确答案），这个技术叫做Teacher Forcing。然而测试的时候，Decoder没有Ground Truth可以使用，Decoder只能使用自己的输入，这里就会产生<strong>Mismatch</strong>（测试集和训练集的数据分布不一致）。</p>
<p>可能的解决方法：给Decoder的输入加一些错误的东西，这个技术叫做Scheduled Sampling<sup id="fnref:12"><a href="#fn:12" rel="footnote">12</a></sup>。</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">Seq2seq在语义解析（Syntactic Parsing）上的应用：  <a href="https://arxiv.org/abs/1412.7449">Grammar as a Foreign Language_1412</a></span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">Seq2seq在多标签分类问题上的应用：<br>
<a href="https://arxiv.org/abs/1909.03434">Order-free Learning Alleviating Exposure Bias in Multi-label Classification_1909</a><br>
<a href="https://arxiv.org/abs/1707.05495">Order-Free RNN with Visual Attention for Multi-Label Classification_1707</a></span><a href="#fnref:2" rev="footnote"> ↩</a></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">3.</span><span style="display: inline-block; vertical-align: top;">Seq2seq在目标检测问题上的应用：  <a href="https://arxiv.org/abs/2005.12872">End-to-End Object Detection with Transformers_2005</a></span><a href="#fnref:3" rev="footnote"> ↩</a></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">4.</span><span style="display: inline-block; vertical-align: top;">Seq2seq模型的提出：<a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks_1409</a></span><a href="#fnref:4" rev="footnote"> ↩</a></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">5.</span><span style="display: inline-block; vertical-align: top;">Transformer模型的提出：<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></span><a href="#fnref:5" rev="footnote"> ↩</a></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">6.</span><span style="display: inline-block; vertical-align: top;">Layer Normalization：<a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></span><a href="#fnref:6" rev="footnote"> ↩</a></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">7.</span><span style="display: inline-block; vertical-align: top;">讨论Layer Norm的位置：<a href="https://arxiv.org/abs/2002.04745">On Layer Normalization in the Transformer Architecture_2002</a><br>
为什么要用layer Norm：<a href="https://arxiv.org/abs/2003.07845">PowerNorm: Rethinking Batch Normalization in Transformers_2003</a></span><a href="#fnref:7" rev="footnote"> ↩</a></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">8.</span><span style="display: inline-block; vertical-align: top;">Decoder和Encoder的不同连接方式：<a href="https://arxiv.org/abs/2005.08081">Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning_2005</a></span><a href="#fnref:8" rev="footnote"> ↩</a></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">9.</span><span style="display: inline-block; vertical-align: top;">生成文章摘要：<a href="https://arxiv.org/abs/1704.04368">Get To The Point: Summarization with Pointer-Generator Networks_1704</a><br>
CopyNet：<a href="https://arxiv.org/abs/1603.06393">Incorporating Copying Mechanism in Sequence-to-Sequence Learning_1603</a></span><a href="#fnref:9" rev="footnote"> ↩</a></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">10.</span><span style="display: inline-block; vertical-align: top;"><a href="https://arxiv.org/abs/1904.09751">The Curious Case of Neural Text Degeneration</a></span><a href="#fnref:10" rev="footnote"> ↩</a></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">11.</span><span style="display: inline-block; vertical-align: top;"><a href="https://arxiv.org/abs/1511.06732">Sequence Level Training with Recurrent Neural Networks</a></span><a href="#fnref:11" rev="footnote"> ↩</a></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">12.</span><span style="display: inline-block; vertical-align: top;">Original Scheduled Sampling：<a href="https://arxiv.org/abs/1506.03099">Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</a><br>
Scheduled Sampling for Transformer：<a href="https://arxiv.org/abs/1906.07651">Scheduled Sampling for Transformers</a><br>
Parallel Scheduled Sampling: <a href="https://arxiv.org/abs/1906.04331">Parallel Scheduled Sampling</a></span><a href="#fnref:12" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>修改hexo生成html文件名的规则</title>
    <url>/modify-the-rules-of-hexo-to-generate-html-file-names.html</url>
    <content><![CDATA[<p>在hexo的配置文件中修改自动生成文件名的规则。<span id="more"></span></p>
<ol>
<li>修改Hexo的根配置文件_config.yml <figure class="highlight yaml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># permalink: :year/:month/:day/:title/</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">:filename.html</span></span><br></pre></td></tr></table></figure>
 <code>:filename.html</code>是HTML的命名格式，其中<code>filename</code>是<code>.md</code>文件中文章头部的变量。<strong>注意</strong>：后缀<code>.html</code>不能省略，否则打开页面时不会跳转到相应页面，而是下载页面。 <figure class="highlight text"><figcaption><span>a.md</span></figcaption><table><tr><td class="code"><pre><span class="line">filename: modify-the-rules-of-hexo-to-generate-html-file-names</span><br></pre></td></tr></table></figure></li>
<li>修改文章模板，在模板中增加<code>filename</code>属性。<br> 文章模板在<code>scaffolds</code>目录中。 <figure class="highlight text"><figcaption><span>scaffolds/post.md</span></figcaption><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags: </span><br><span class="line">    - </span><br><span class="line">categories: </span><br><span class="line">    - </span><br><span class="line">keywords: </span><br><span class="line">    - </span><br><span class="line">filename: </span><br><span class="line">---</span><br></pre></td></tr></table></figure></li>
</ol>
<p>【参考资料】</p>
<ol>
<li><a href="https://cloud.tencent.com/developer/article/1463509">修改Hexo自动生成的HTML文件名</a></li>
</ol>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow2.0实现线性回归</title>
    <url>/tf-linearregression.html</url>
    <content><![CDATA[<p>使用<code>TensorFlow</code>实现一个简单的线性回归模型，以熟悉<code>TensorFlow</code>的使用方法。参考<a href="https://www.tensorflow.org/guide/basic_training_loops">官方文档</a>。</p>
<span id="more"></span>
<p>实验环境：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Tensorflow Version:  2.4.1</span><br><span class="line">Python Version:  3.7.10</span><br></pre></td></tr></table></figure>
<h1 id="生成随机数据集"><a href="#生成随机数据集" class="headerlink" title="生成随机数据集"></a>生成随机数据集</h1><p>设训练数据集样本数为1000，每个样本的特征数为1。令w=0.6, b=0.4，随机噪声为服从均值为0、标准差为1的正态分布。噪声代表了数据集中无意义的干扰。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 实际的线</span></span><br><span class="line">TRUE_W = <span class="number">0.6</span></span><br><span class="line">TRUE_B = <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line">NUM_EXAMPLES = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"><span class="comment"># 随机向量x</span></span><br><span class="line">x = tf.random.normal(shape=[NUM_EXAMPLES])</span><br><span class="line"><span class="comment"># 生成噪声</span></span><br><span class="line">noise = tf.random.normal(shape=[NUM_EXAMPLES])</span><br><span class="line"><span class="comment"># 计算y</span></span><br><span class="line">y = x * TRUE_W + TRUE_B + noise</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;features shape：&quot;</span>, x.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;labels shape：&quot;</span>, y.shape)</span><br><span class="line"><span class="built_in">print</span>(features[<span class="number">0</span>].numpy(), labels[<span class="number">0</span>].numpy())</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">features shape： (1000,)</span><br><span class="line">labels shape： (1000,)</span><br><span class="line">[0.5983449  0.06276207] [2.968863]</span><br></pre></td></tr></table></figure>
<h1 id="定义回归模型"><a href="#定义回归模型" class="headerlink" title="定义回归模型"></a>定义回归模型</h1><p>$f(w, b, x) = w * x + b$<br>构建自定义模型类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">tf.Module</span>):</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 随机初始化w、b</span></span><br><span class="line">        self.w = tf.Variable(tf.random.uniform([<span class="number">1</span>]))</span><br><span class="line">        self.b = tf.Variable(tf.random.uniform([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.w * x + self.b</span><br><span class="line"></span><br><span class="line">model = MyModel()</span><br></pre></td></tr></table></figure>
<p>Keras中附带的 <a href="https://tensorflow.google.cn/api_docs/python/tf/keras/initializers">初始化器</a>。  </p>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><p>使用均方误差。$J = \frac{1}{n}\sum_{i=1}^n(y_i-f(x_i))^2$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算整个批次的单个损失值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">target_y, predicted_y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.square(target_y - predicted_y))</span><br></pre></td></tr></table></figure>
<p>绘制测试数据集的散点图以及随机初始化的参数拟合的直线。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(x.numpy(), y.numpy(), <span class="number">1</span>)</span><br><span class="line">plt.plot(x.numpy(), model(x), c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Current loss: %1.6f&quot;</span> % loss(y, model(x)).numpy())</span><br></pre></td></tr></table></figure>
<p><img src="data:image/png;base64,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" data-original="./fig1.jpg"></p>
<h2 id="定义训练循环"><a href="#定义训练循环" class="headerlink" title="定义训练循环"></a>定义训练循环</h2><p>训练循环按顺序重复执行以下任务：</p>
<ul>
<li>发送一批输入值，通过模型生成输出值</li>
<li>通过比较输出值与输出（标签），来计算损失值</li>
<li>使用<a href="https://tensorflow.google.cn/api_docs/python/tf/GradientTape">梯度带(GradientTape)</a>找到梯度值</li>
<li>使用这些梯度优化变量</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给定一个可调用的模型，输入，输出和学习率...</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model, x, y, learning_rate</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> t:</span><br><span class="line">        <span class="comment"># 可训练变量由GradientTape自动跟踪</span></span><br><span class="line">        current_loss = loss(y, model(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用GradientTape计算相对于W和b的梯度</span></span><br><span class="line">    dw, db = t.gradient(current_loss, [model.w, model.b])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 减去由学习率缩放的梯度</span></span><br><span class="line">    model.w.assign_sub(learning_rate * dw)</span><br><span class="line">    model.b.assign_sub(learning_rate * db)</span><br><span class="line">    </span><br><span class="line">model = MyModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 收集W值和b值的历史记录以供以后绘制</span></span><br><span class="line">Ws, bs = [], []</span><br><span class="line">epochs = <span class="built_in">range</span>(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义用于训练的循环</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">model, x, y</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">        <span class="comment"># 用单个大批次处理更新模型</span></span><br><span class="line">        train(model, x, y, learning_rate=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在更新之前进行跟踪</span></span><br><span class="line">        Ws.append(model.w.numpy())</span><br><span class="line">        bs.append(model.b.numpy())</span><br><span class="line">        current_loss = loss(y, model(x))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f&quot;</span> %</span><br><span class="line">              (epoch, Ws[-<span class="number">1</span>], bs[-<span class="number">1</span>], current_loss))</span><br><span class="line">              </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Starting: W=%1.2f b=%1.2f, loss=%2.5f&quot;</span> %</span><br><span class="line">      (model.w, model.b, loss(y, model(x))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">training_loop(model, x, y)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>线性回归</tag>
        <tag>LinearRegression</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow基础操作</title>
    <url>/tf-prerequisite.html</url>
    <content><![CDATA[<p>介绍Tensorflow的一些基础操作。</p>
<span id="more"></span>
<h1 id="张量Tensor"><a href="#张量Tensor" class="headerlink" title="张量Tensor"></a>张量Tensor</h1><p>张量与numpy的<code>np.arrays</code>相似。就像 Python 数值和字符串一样，所有张量都是不可变的。详细介绍参考<a href="https://www.tensorflow.org/guide/tensor">官方文档</a>。</p>
<h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>1、创建一个基本的张量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = tf.constant([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">tf.Tensor([2 3 4], shape=(3,), dtype=int32)</span><br></pre></td></tr></table></figure>
<p>2、创建随机张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置随机种子</span></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)</span><br><span class="line"><span class="comment"># 创建符合正态分布（高斯分布）的随机值</span></span><br><span class="line">a = tf.random.normal((<span class="number">2</span>, <span class="number">3</span>), mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 在第一个维度上随机打乱一个张量</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">b = tf.random.shuffle(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># 创建符合均匀分布的随机量</span></span><br><span class="line">c = tf.random.uniform((<span class="number">2</span>, <span class="number">3</span>), minval=<span class="number">0</span>, maxval=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[ 0.5983449   0.06276207  0.14631742]</span><br><span class="line"> [ 0.48481876 -0.23572342  0.61806035]], shape=(2, 3), dtype=float32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[3 4]</span><br><span class="line"> [1 2]</span><br><span class="line"> [5 6]], shape=(3, 2), dtype=int32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[0.25153685 0.30157518 0.5093776 ]</span><br><span class="line"> [0.6036475  0.9555522  0.3162737 ]], shape=(2, 3), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>3、创建序列</p>
<h2 id="tensor转numpy"><a href="#tensor转numpy" class="headerlink" title="tensor转numpy"></a>tensor转numpy</h2><p>通过使用 <code>np.array</code> 或 <code>tensor.numpy</code> 方法，您可以将张量转换为 NumPy 数组：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.array(tensor)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">array([2, 3, 4], dtype=int32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor.numpy()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">array([2, 3, 4], dtype=int32)</span><br></pre></td></tr></table></figure>
<h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><h3 id="基本数学运算"><a href="#基本数学运算" class="headerlink" title="基本数学运算"></a>基本数学运算</h3><p>可以对张量执行基本数学运算，包括加法、逐元素乘法和矩阵乘法运算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.add(a, b))  <span class="comment"># print(a + b) 逐元素相加</span></span><br><span class="line"><span class="built_in">print</span>(tf.multiply(a, b))  <span class="comment"># print(a * b) 逐元素相乘</span></span><br><span class="line"><span class="built_in">print</span>(tf.matmul(a, b))  <span class="comment"># print(a @ b) 矩阵乘法</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[3 3]</span><br><span class="line"> [3 3]], shape=(2, 2), dtype=int32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[2 2]</span><br><span class="line"> [2 2]], shape=(2, 2), dtype=int32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[4 5]</span><br><span class="line"> [5 4]], shape=(2, 2), dtype=int32)</span><br></pre></td></tr></table></figure>
<h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>TensorFlow 遵循标准 Python 索引规则以及 NumPy 索引的基本规则。</p>
<ul>
<li>索引从 <code>0</code> 开始</li>
<li>负索引表示按倒序编制索引</li>
<li>冒号 <code>:</code> 用于切片 <code>start:stop:step</code></li>
</ul>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>tensor</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo搭建个人博客</title>
    <url>/use-hexo-to-build-a-personal-blog.html</url>
    <content><![CDATA[<p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。更多介绍参考<a href="https://hexo.io/zh-cn/docs/">官方文档</a>。<span id="more"></span></p>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><ol>
<li>hexo clean<br>清除缓存文件<code>db.json</code>和已生成的静态文件<code>public</code>, 网站显示异常时可以执行这条命令试试</li>
<li>hexo s<br>启动本地服务器，用于预览主题。默认地址： <a href="http://localhost:4000/">http://localhost:4000/</a></li>
<li>hexo new [name]<br>新建一篇标题为 [name] 的文章,文件一般在<code>source/_post</code>文件夹下</li>
<li>hexo g<br>生成网站静态文件到默认设置的<code>public</code>文件夹</li>
<li>hexo d<br>自动生成网站静态文件，并部署到设定的仓库。</li>
</ol>
<p>在项目根目录下打开<code>git bash</code>，使用以下命令部署项目：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g -d</span><br></pre></td></tr></table></figure>
<h1 id="写博客"><a href="#写博客" class="headerlink" title="写博客"></a>写博客</h1><p>推荐使用<code>HexoEditor</code>编辑器写博客。在这里<a href="https://github.com/zhuzhuyule/HexoEditor/releases">下载</a>安装包，使用方法参考<a href="https://akilar.top/posts/1da4f99e/">这篇文章</a>。</p>
<ol>
<li>写文章。<br>打开菜单栏，选择【新建Post】（快捷键<code>Ctr+H</code>），使用<code>post模板</code>新建一个文件。</li>
<li>部署。<br>step 1:【编辑窗口右键】-&gt;【Hexo】-&gt;【清除缓存文件】<br>step 2:【编辑窗口右键】-&gt;【Hexo】-&gt;【一键部署】</li>
</ol>
<h1 id="hexo-插件"><a href="#hexo-插件" class="headerlink" title="hexo 插件"></a>hexo 插件</h1><h2 id="博客优化"><a href="#博客优化" class="headerlink" title="博客优化"></a>博客优化</h2><p>博客压缩：hexo-neat<br>图片懒加载：hexo-lazyload-image<br><a href="https://evanhongyousan.github.io/2020/06/26/hexo-next-github-page-performance/">配置使用jsDelivr cdn、图片懒加载</a><br>安装插件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-neat --save</span><br></pre></td></tr></table></figure>
<p>修改配置文件</p>
<figure class="highlight yaml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># 博文压缩</span></span><br><span class="line"><span class="attr">neat_enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 压缩html</span></span><br><span class="line"><span class="attr">neat_html:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">exclude:</span></span><br><span class="line"><span class="comment"># 压缩css  </span></span><br><span class="line"><span class="attr">neat_css:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">exclude:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;**/*.min.css&#x27;</span></span><br><span class="line"><span class="comment"># 压缩js</span></span><br><span class="line"><span class="attr">neat_js:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">mangle:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">output:</span></span><br><span class="line">  <span class="attr">compress:</span></span><br><span class="line">  <span class="attr">exclude:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;**/*.min.js&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;**/jquery.fancybox.pack.js&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;**/index.js&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;**/fireworks.js&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="sitemap生成"><a href="#sitemap生成" class="headerlink" title="sitemap生成"></a>sitemap生成</h2><p>google：hexo-generator-sitemap<br>baidu：hexo-generator-baidu-sitemap<br>详细配置参考：<a href="https://eericzeng.github.io/2019/07/14/hexo%E5%8D%9A%E5%AE%A2%E7%AB%99%E7%82%B9sitemap%E7%9A%84%E4%BD%BF%E7%94%A8/">hexo博客站点sitemap的使用</a></p>
<h2 id="生成短链接"><a href="#生成短链接" class="headerlink" title="生成短链接"></a>生成短链接</h2><p>hexo-abbrlink<br>安装插件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-abbrlink --save</span><br></pre></td></tr></table></figure>
<p>修改配置文件</p>
<figure class="highlight yaml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># permalink: :year/:month/:day/:title/</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">:abbrlink/</span></span><br><span class="line"><span class="attr">abbrlink:</span></span><br><span class="line">  <span class="attr">alg:</span> <span class="string">crc16</span>  <span class="comment"># 算法：crc16(default) and crc32</span></span><br><span class="line">  <span class="attr">rep:</span> <span class="string">hex</span>    <span class="comment"># 进制：dec(default) and hex</span></span><br></pre></td></tr></table></figure>

<h1 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h1><h2 id="文章中添加图片"><a href="#文章中添加图片" class="headerlink" title="文章中添加图片"></a>文章中添加图片</h2><p>图片可以放在文章自己的目录中。文章的目录可以通过配置_config.yml来生成。</p>
<figure class="highlight yaml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">post_asset_folder:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>将<code>_config.yml</code>文件中的配置项<code>post_asset_folder</code>设为<code>true</code>后，执行命令<code>hexo new post_name</code>，在<code>source/_posts</code>中会生成文章<code>post_name.md</code>和同名文件夹<code>post_name</code>。将图片资源放在post_name中，文章就可以使用相对路径引用图片资源了。</p>
<figure class="highlight text"><figcaption><span>post_name.md</span></figcaption><table><tr><td class="code"><pre><span class="line">![self-attention结构](.\xxx.png#pic_center)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>CDN引用</strong><br>除了在本地存储图片，还可以将图片上传到一些免费的CDN服务中。在CDN中上传图片后，会生成对应的url地址，将地址直接拿来引用即可。</p>
</blockquote>
<h2 id="修改文章模板"><a href="#修改文章模板" class="headerlink" title="修改文章模板"></a>修改文章模板</h2><p>模板文件在hexo根目录下的<code>scaffolds</code>目录中：<code>draft.md</code>、<code>page.md</code>和<code>post.md</code>。</p>
<p>参考资料：</p>
<ol>
<li><a href="https://segmentfault.com/a/1190000023346633">保姆级——小白如何搭建自己的博客（Hexo+Github Pages）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/109382792">如何快速搭建自己的博客平台</a></li>
</ol>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>使用matplotlib绘制子图</title>
    <url>/use-matplotlib-to-draw-subplots.html</url>
    <content><![CDATA[<p>使用matplotlib绘制子图。<span id="more"></span></p>
<h1 id="Figure-subplots"><a href="#Figure-subplots" class="headerlink" title="Figure.subplots"></a>Figure.subplots</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=[<span class="number">6.4</span>, <span class="number">4.8</span>])</span><br><span class="line">x = np.array(<span class="built_in">range</span>(-<span class="number">5</span>, <span class="number">6</span>))</span><br></pre></td></tr></table></figure>
<h2 id="创建一行子图"><a href="#创建一行子图" class="headerlink" title="创建一行子图"></a>创建一行子图</h2><p>方式一：使用索引方式引用子图。这种方式适用于子图<strong>图形相同数据不同</strong>的情况。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax = fig.subplots(<span class="number">1</span>, <span class="number">2</span>, sharey=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    ax[i].plot(x, x**<span class="number">2</span>, label=i)  <span class="comment"># 绘制折线图</span></span><br><span class="line">    ax[i].legend()  <span class="comment"># 显示图例，画图时需要指定label</span></span><br><span class="line">    ax[i].set_xticks(x)  <span class="comment"># 设置x轴刻度</span></span><br></pre></td></tr></table></figure>
<p>方式二：创建子图时对返回值进行解包。这种方式适用于子图<strong>图形不同</strong>的情况。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax1, ax2 = fig.subplots(<span class="number">1</span>, <span class="number">2</span>, sharey=<span class="literal">True</span>)</span><br><span class="line">ax1.plot(x, x**<span class="number">2</span>)</span><br><span class="line">ax2.scatter(x, x**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h2 id="创建多行子图"><a href="#创建多行子图" class="headerlink" title="创建多行子图"></a>创建多行子图</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax = fig.subplots(<span class="number">2</span>, <span class="number">3</span>, sharex=<span class="string">&#x27;col&#x27;</span>, sharey=<span class="string">&#x27;row&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># axes are in a two-dimensional array, indexed by [row, col]</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        ax[i, j].text(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="built_in">str</span>((i, j)),</span><br><span class="line">                      fontsize=<span class="number">18</span>, ha=<span class="string">&#x27;center&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注意多行子图和一行子图索引的写法。</strong><br>更多用法参考<a href="https://matplotlib.org/stable/api/figure_api.html?highlight=subplots#matplotlib.figure.Figure.subplots">官方文档</a>。</p>
<h1 id="Figure-add-subplot"><a href="#Figure-add-subplot" class="headerlink" title="Figure.add_subplot"></a>Figure.add_subplot</h1><p>向画布中指定网格添加子图。没有添加子图的地方保持空白。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax1 = fig.add_subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>) </span><br><span class="line">ax1.plot(x, x)</span><br><span class="line"><span class="comment"># ax2 = fig.add_subplot(2, 3, 2) </span></span><br><span class="line"><span class="comment"># ax2.plot(x, x**2)</span></span><br><span class="line"><span class="comment"># ax3 = fig.add_subplot(2, 3, 3) </span></span><br><span class="line"><span class="comment"># ax3.plot(x, x**3)</span></span><br><span class="line">ax4 = fig.add_subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) </span><br><span class="line">ax4.plot(x, np.log(x))</span><br><span class="line">ax5 = fig.add_subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>) </span><br><span class="line">ax5.plot(x, np.exp(x))</span><br></pre></td></tr></table></figure>
<p>更多用法参考<a href="https://matplotlib.org/stable/api/figure_api.html?highlight=subplots#matplotlib.figure.Figure.add_subplot">官方文档</a>。</p>
<p>【参考资料】</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/75276939">Matplotlib 多子图绘制</a></li>
</ol>
]]></content>
      <categories>
        <category>matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>subplots</tag>
      </tags>
  </entry>
</search>
