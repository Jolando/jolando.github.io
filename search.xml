<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>使用hexo搭建个人博客</title>
    <url>/2021/04/29/index/</url>
    <content><![CDATA[<h4 id="启动本地服务器"><a href="#启动本地服务器" class="headerlink" title="启动本地服务器"></a>启动本地服务器</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>指定端口：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo s -p 9000</span><br></pre></td></tr></table></figure><br><span id="more"></span></p>
<h4 id="使用hexo-admin写文章"><a href="#使用hexo-admin写文章" class="headerlink" title="使用hexo admin写文章"></a>使用hexo admin写文章</h4><p>启动本地服务器后，切换链接：<a href="http://localhost:4000/admin">http://localhost:4000/admin</a></p>
<h4 id="本地修改部署到github"><a href="#本地修改部署到github" class="headerlink" title="本地修改部署到github"></a>本地修改部署到github</h4><p>修改本地配置文件或者编写文章后，需要使用以下命令部署到github。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo d -g</span><br></pre></td></tr></table></figure></p>
<p>参考资料：</p>
<ol>
<li><a href="https://segmentfault.com/a/1190000023346633">保姆级——小白如何搭建自己的博客（Hexo+Github Pages）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/109382792">如何快速搭建自己的博客平台</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>李宏毅机器学习学习笔记——自注意力机制</title>
    <url>/2021/04/30/lhy-self-attention/</url>
    <content><![CDATA[<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><p>self-attention也是一个常见的neural network框架，用来解决输入是一组向量并且向量的个数是不确定的问题。例如：</p>
<ul>
<li>文本处理</li>
<li>语音识别</li>
<li>Drug discovery<span id="more"></span>
self-attention的输出有以下3种情况：<ol>
<li><strong>每个向量都有一个标签</strong>，即输出跟输入数目一样。具体应用：词性标注、推荐系统。</li>
<li><strong>整个sequence只输出一个标签</strong>。具体应用：语义分析、说话人识别、图片（分子结构图）识别。</li>
<li><strong>输出的标签个数不确定</strong>。具体应用：机器翻译。</li>
</ol>
</li>
</ul>
<h1 id="类型一：输出label与输入向量个数相同（Sequence-Labeling）"><a href="#类型一：输出label与输入向量个数相同（Sequence-Labeling）" class="headerlink" title="类型一：输出label与输入向量个数相同（Sequence Labeling）"></a>类型一：输出label与输入向量个数相同（Sequence Labeling）</h1><p>对于输出与输入数目相同的情况，例如对句子<code>I saw a saw</code>中的单词进行词性标注，在这句话中，第一个<code>saw</code>是动词，第二个<code>saw</code>是名词。如果使用全连接网络来处理，有两种方法：</p>
<ol>
<li><strong>逐个向量处理</strong>。这种方法的缺陷是，它无法利用序列的上下文信息，全连接网络对于任意位置<code>saw</code>的词性预测结果必然是一样的。</li>
<li><strong>考虑上下文信息</strong>，即在对<code>saw</code>进行词性标注时，设置一个window同时将前面几个单词和后面几个单词考虑进来。但是由于句子长度的不确定性，window设置过小，则不能将长句子中的所有单词考虑进来；window设置过大，又会使得网络参数太多，导致过拟合。</li>
</ol>
<p>为了将整个句子的信息考虑进来，就要用到self-attention技术。<br><img src="lhy-self-attention\self-attention.png#pic_center" alt="self-attention结构"><br>self-attention对于每个向量都会考虑整个sequence的信息后输出一个向量。</p>
<h2 id="self-attention——计算输出向量"><a href="#self-attention——计算输出向量" class="headerlink" title="self-attention——计算输出向量"></a>self-attention——计算输出向量</h2><p>在self-attention中，计算attention的步骤：<br><img src="lhy-self-attention\attention_compute.png#pic_center" alt="attention计算步骤"></p>
<ol>
<li>计算 $query$，即当前向量与 $W_{q}$ 的乘积。计算方法 ：$q_{i} = a_{i}W_{q}$。</li>
<li>计算 $key$，即其他向量与 $W_{k}$ 的乘积 。计算方法：$k_{j} = a_{j}W_{k},  k\in[1, n]$。</li>
<li>计算attetion score ($\alpha$)，即当前向量与其他向量的相关性。计算方法：$\alpha_{i, j} = q_{i}.k_{j}$。<em>通常情况下，也需要计算向量和自己的相关性</em> 。</li>
<li>通过softmax函数激活$\alpha$得到$\alpha’$（也可以使用其他激活函数）。$\alpha’_{i, j} = softmax(\alpha_{i, j})$。</li>
<li>计算 $value$ 。根据$\alpha’$抽取sequence中重要信息。计算方法 ：$v_{j} = a_{j}W_{v}$</li>
<li>计算输出向量$b_{i}$。计算方法 ：$b_{i} =\sum_{j=1}^n\alpha’_{i, j}.v_{j}$。</li>
</ol>
<blockquote>
<p>计算两个向量相关性的方法：</p>
<ol>
<li><strong>点乘运算（dot product）</strong>。最常用的方法。输入向量分别乘上两个不同的矩阵 $W_{q}$ 和 $W_{k}$ 得到向量 $q$ 和 $k$ ，再把 $q$ 和 $k$ 做点乘。 $a_{i}$ 和  $a_{j}$ 的相关度 $\alpha_{i,j} = (a_{i}W_{q}) . (a_{j}W_{k})$ 。</li>
<li>加性运算（additive）。不对 $q$ 和 $k$ 做点乘，而是串联起来后使用 $tanh$ 函数激活。$a_{i}$ 和  $a_{j}$ 的相关度 $\alpha_{i,j} = tanh((a_{i}W_{q}) + (a_{j}W_{k}))$ 。</li>
</ol>
</blockquote>
<p>参考资料：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/53682800">nlp中的Attention注意力机制+Transformer详解</a></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>self-attention</tag>
        <tag>自注意力</tag>
      </tags>
  </entry>
</search>
